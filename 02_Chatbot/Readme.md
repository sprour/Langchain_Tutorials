## Создание чат-бота

### Обзор

Мы рассмотрим пример проектирования и реализации чат-бота на основе LLM. Этот чат-бот сможет вести разговор и запоминать предыдущие взаимодействия.

Обратите внимание, что этот чат-бот, который мы создадим, будет использовать языковую модель только для ведения разговора. Существует несколько других связанных концепций, которые могут вас заинтересовать:

* **Conversational RAG:** предоставление чат-боту возможности доступа к внешнему источнику данных.
* **Агенты:** создание чат-бота, который может выполнять действия.

Данное руководство охватывает основы, которые будут полезны для этих двух более продвинутых тем, но вы можете сразу перейти к ним, если хотите.

### Концепции

Вот некоторые высокоуровневые компоненты, с которыми мы будем работать:

* **Чат-модели (Chat Models).** Интерфейс чат-бота основан на сообщениях, а не на необработанном тексте, и поэтому лучше всего подходит для чат-моделей, а не для текстовых LLM.
* **Шаблоны подсказок (Prompt Templates),** которые упрощают процесс сборки подсказок, объединяющих сообщения по умолчанию, пользовательский ввод, историю чата и (опционально) дополнительный извлеченный контекст.
* **История чата (Chat History),** которая позволяет чат-боту «запоминать» прошлые взаимодействия и учитывать их при ответе на последующие вопросы.
* **Отладка и трассировка вашего приложения с помощью LangSmith**

Мы рассмотрим, как объединить вышеперечисленные компоненты для создания мощного диалогового чат-бота.

### Настройка

#### Jupyter Notebook

Это руководство (и большинство других руководств в документации) использует **Jupyter Notebook**, и предполагается, что читатель тоже его использует. Jupyter Notebook идеально подходит для изучения работы с LLM-системами, потому что часто что-то может пойти не так (неожиданный вывод, недоступность API и т. д.), и прохождение руководств в интерактивной среде — отличный способ лучше их понять.

Этот и другие учебные пособия, пожалуй, удобнее всего запускать в Jupyter Notebook. Инструкции по установке см. **здесь**.

#### Установка

Чтобы установить LangChain, выполните:

```bash
pip install langchain
```

или

```bash
conda install langchain -c conda-forge
```

Подробнее см. наше **руководство по установке**.

#### LangSmith

Многие приложения, которые вы создаете с помощью LangChain, будут содержать несколько шагов с несколькими вызовами LLM. По мере того как эти приложения становятся все более сложными, становится критически важным иметь возможность проверять, что именно происходит внутри вашей цепочки или агента. Лучший способ сделать это — использовать **LangSmith**.

После регистрации по ссылке выше обязательно настройте переменные среды, чтобы начать запись трассировок:

```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

Или, если вы работаете в блокноте, вы можете установить их следующим образом:

```python
import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

### Быстрый старт

Во-первых, давайте узнаем, как использовать языковую модель саму по себе. LangChain поддерживает множество различных языковых моделей, которые вы можете использовать взаимозаменяемо — выберите ту, которую вы хотите использовать ниже!

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-3.5-turbo")
```

Давайте сначала используем модель напрямую. **ChatModels** — это экземпляры «выполняемых объектов» (Runnables) LangChain, что означает, что они предоставляют стандартный интерфейс для взаимодействия с ними. Чтобы просто вызвать модель, мы можем передать список сообщений в метод **.invoke**.

```python
from langchain_core.messages import HumanMessage

model.invoke([HumanMessage(content="Привет! Я Боб")])
```

Модель сама по себе не имеет никакого представления о состоянии. Например, если вы зададите следующий вопрос:

```python
model.invoke([HumanMessage(content="Как меня зовут?")])
```

Давайте взглянем на пример **трассировки LangSmith**. Мы видим, что она не принимает предыдущий поворот разговора в контекст и не может ответить на вопрос. Это создает ужасный пользовательский опыт чат-бота!

Чтобы обойти это, нам нужно передать всю историю разговора в модель. Давайте посмотрим, что произойдет, когда мы это сделаем:

```python
from langchain_core.messages import AIMessage

model.invoke(
[
HumanMessage(content="Привет! Я Боб"),
AIMessage(content="Привет, Боб! Чем я могу тебе помочь сегодня?"),
HumanMessage(content="Как меня зовут?"),
]
)
```

И теперь мы видим, что получаем хороший ответ!

Это основная идея, лежащая в основе способности чат-бота взаимодействовать в режиме разговора. Так как же нам лучше всего это реализовать?

### История сообщений

Мы можем использовать класс **Message History**, чтобы обернуть нашу модель и сделать ее stateful (с состоянием). Он будет отслеживать входные и выходные данные модели и хранить их в каком-либо хранилище данных. Последующие взаимодействия будут загружать эти сообщения и передавать их в цепочку как часть входных данных. Давайте посмотрим, как это использовать!

Сначала давайте убедимся, что установили **langchain-community**, так как мы будем использовать интеграцию, которая там есть, для хранения истории сообщений.

```bash
pip install langchain_community
```

После этого мы можем импортировать соответствующие классы и настроить нашу цепочку, которая обертывает модель и добавляет эту историю сообщений. Ключевой частью здесь является функция, которую мы передаем как **get_session_history**. Ожидается, что эта функция будет принимать **session_id** и возвращать объект **Message History**. Этот **session_id** используется для различения отдельных разговоров, и его следует передавать как часть конфигурации при вызове новой цепочки (мы покажем, как это сделать).

```python
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
if session_id not in store:
store[session_id] = ChatMessageHistory()
return store[session_id]


with_message_history = RunnableWithMessageHistory(model, get_session_history)
```

Теперь нам нужно создать **конфигурацию**, которую мы будем передавать в runnable каждый раз. Эта конфигурация содержит информацию, которая не является частью входных данных напрямую, но все же полезна. В данном случае мы хотим включить **session_id**. Это должно выглядеть так:

```python
config = {"configurable": {"session_id": "abc2"}}
```

```python
response = with_message_history.invoke(
[HumanMessage(content="Привет! Я Боб")],
config=config,
)

response.content
```

```python
response = with_message_history.invoke(
[HumanMessage(content="Как меня зовут?")],
config=config,
)

response.content
```

Отлично! Наш чат-бот теперь помнит о нас. Если мы изменим конфигурацию, указав другой **session_id**, мы увидим, что он начинает разговор заново.

```python
config = {"configurable": {"session_id": "abc3"}}

response = with_message_history.invoke(
[HumanMessage(content="Как меня зовут?")],
config=config,
)

response.content
```

Однако мы всегда можем вернуться к исходному разговору (поскольку мы сохраняем его в базе данных)

```python
config = {"configurable": {"session_id": "abc2"}}

response = with_message_history.invoke(
[HumanMessage(content="Как меня зовут?")],
config=config,
)

response.content
```

Вот как мы можем поддержать чат-бота, ведущего разговоры со многими пользователями!

На данный момент мы просто добавили простой уровень сохраняемости вокруг модели. Мы можем начать делать ее более сложной и персонализированной, добавив шаблон подсказки.

### Шаблоны подсказок

Шаблоны подсказок помогают преобразовать необработанную пользовательскую информацию в формат, с которым может работать LLM. В данном случае необработанный пользовательский ввод — это просто сообщение, которое мы передаем в LLM. Давайте теперь сделаем это немного сложнее. Сначала давайте добавим системное сообщение с некоторыми пользовательскими инструкциями (но все еще принимая сообщения в качестве входных данных). Затем мы добавим больше входных данных, помимо просто сообщений.

Во-первых, давайте добавим системное сообщение. Для этого мы создадим **ChatPromptTemplate**. Мы будем использовать **MessagesPlaceholder**, чтобы передать все сообщения.

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
[
(
"system",
"Ты — полезный ассистент. Отвечай на все вопросы в меру своих возможностей.",
),
MessagesPlaceholder(variable_name="messages"),
]
)

chain = prompt | model
```

Обратите внимание, что это немного меняет тип входных данных — вместо того, чтобы передавать список сообщений, мы теперь передаем словарь с ключом **messages**, где он содержит список сообщений.

```python
response = chain.invoke({"messages": [HumanMessage(content="Привет! Я Боб")]})

response.content
```

Теперь мы можем обернуть это в тот же объект **Messages History**, что и раньше.

```python
with_message_history = RunnableWithMessageHistory(chain, get_session_history)
```

```python
config = {"configurable": {"session_id": "abc5"}}
```

```python
response = with_message_history.invoke(
[HumanMessage(content="Привет! Я Джим")],
config=config,
)

response.content
```

```python
response = with_message_history.invoke(
[HumanMessage(content="Как меня зовут?")],
config=config,
)

response.content
```

Потрясающе! Давайте теперь сделаем нашу подсказку немного сложнее. Предположим, что шаблон подсказки теперь выглядит следующим образом:

```python
prompt = ChatPromptTemplate.from_messages(
[
(
"system",
"Ты — полезный ассистент. Отвечай на все вопросы в меру своих возможностей на {language}.",
),
MessagesPlaceholder(variable_name="messages"),
]
)

chain = prompt | model
```

Обратите внимание, что мы добавили новый входной параметр **language** в подсказку. Теперь мы можем вызвать цепочку и передать язык по нашему выбору.

```python
response = chain.invoke(
{"messages": [HumanMessage(content="Привет! Я Боб")], "language": "Spanish"}
)

response.content
```

Теперь давайте обернем эту более сложную цепочку в класс **Message History**. На этот раз, поскольку во входных данных есть несколько ключей, нам нужно указать правильный ключ, который будет использоваться для сохранения истории чата.

```python
with_message_history = RunnableWithMessageHistory(
chain,
get_session_history,
input_messages_key="messages",
)
```

```python
config = {"configurable": {"session_id": "abc11"}}
```

```python
response = with_message_history.invoke(
{"messages": [HumanMessage(content="Привет! Я Тодд")], "language": "Spanish"},
config=config,
)

response.content
```

```python
response = with_message_history.invoke(
{"messages": [HumanMessage(content="Как меня зовут?")], "language": "Spanish"},
config=config,
)

response.content
```

Чтобы помочь вам понять, что происходит внутри, ознакомьтесь с этой **трассировкой LangSmith**.

### Управление историей разговоров

Одна важная концепция, которую нужно понимать при создании чат-ботов, — это управление историей разговоров. Если ее не контролировать, список сообщений будет расти неограниченно и потенциально переполнит контекстное окно LLM. Поэтому важно добавить шаг, ограничивающий размер передаваемых сообщений.

Важно отметить, что вам нужно будет сделать это **ДО** шаблона подсказки, но **ПОСЛЕ** того, как вы загрузите предыдущие сообщения из **Message History**.

Мы можем сделать это, добавив простой шаг перед подсказкой, который соответствующим образом изменит ключ **messages**, а затем обернем эту новую цепочку в класс **Message History**. Сначала давайте определим функцию, которая будет изменять передаваемые сообщения. Давайте сделаем так, чтобы она выбирала **k** самых последних сообщений. Затем мы можем создать новую цепочку, добавив ее в начало.

```python
from langchain_core.runnables import RunnablePassthrough

def filter_messages(messages, k=10):
return messages[-k:]


chain = (
RunnablePassthrough.assign(messages=lambda x: filter_messages(x["messages"])) | prompt | model
)
```

Давайте теперь попробуем! Если мы создадим список сообщений длиной более 10 сообщений, мы увидим, что он больше не помнит информацию в ранних сообщениях.

```python
messages = [
HumanMessage(content="Привет! Я Боб"),
AIMessage(content="Привет!"),
HumanMessage(content="Мне нравится ванильное мороженое"),
AIMessage(content="Хорошо"),
HumanMessage(content="Сколько будет 2 + 2"),
AIMessage(content="4"),
HumanMessage(content="Спасибо"),
AIMessage(content="Пожалуйста!"),
HumanMessage(content="Тебе весело?"),
AIMessage(content="Да!"),
]
```

```python
response = chain.invoke(
{
"messages": messages + [HumanMessage(content="Как меня зовут?")],
"language": "English",
}
)
response.content
```

Но если мы спросим об информации, которая находится в последних десяти сообщениях, он все еще помнит ее.

```python
response = chain.invoke(
{
"messages": messages + [HumanMessage(content="Какое мое любимое мороженое?")],
"language": "English",
}
)
response.content
```

Теперь давайте обернем это в **Message History**.

```python
with_message_history = RunnableWithMessageHistory(
chain,
get_session_history,
input_messages_key="messages",
)

config = {"configurable": {"session_id": "abc20"}}
```

```python
response = with_message_history.invoke(
{
"messages": messages + [HumanMessage(content="Как меня зовут?")],
"language": "English",
},
config=config,
)

response.content
```

Теперь в истории чата есть два новых сообщения. Это означает, что еще больше информации, которая раньше была доступна в нашей истории разговоров, теперь недоступна!

```python
response = with_message_history.invoke(
{
"messages": [HumanMessage(content="Какое мое любимое мороженое?")],
"language": "English",
},
config=config,
)

response.content
```

Если вы посмотрите на LangSmith, вы можете увидеть, что именно происходит под капотом, в **трассировке LangSmith**.

### Потоковая передача

Теперь у нас есть работающий чат-бот. Тем не менее, одним действительно важным фактором UX для чат-ботов является потоковая передача. LLM иногда может потребоваться некоторое время, чтобы ответить, и поэтому для улучшения пользовательского опыта большинство приложений делают потоковую передачу каждого токена по мере его генерации. Это позволяет пользователю видеть прогресс.

На самом деле это очень просто сделать!

Все цепочки предоставляют метод **.stream**, и те, которые используют историю сообщений, ничем не отличаются. Мы можем просто использовать этот метод, чтобы получить потоковый ответ.

```python
config = {"configurable": {"session_id": "abc15"}}
for r in with_message_history.stream(
{
"messages": [HumanMessage(content="Привет! Я Тодд. Расскажи мне шутку")],
"language": "English",
},
config=config,
):
print(r.content, end="|")
```

### Следующие шаги

Теперь, когда вы понимаете основы создания чат-бота в LangChain, вот несколько более продвинутых учебных пособий, которые могут вас заинтересовать:

* **Conversational RAG:** предоставление чат-боту возможности доступа к внешнему источнику данных.
* **Агенты:** создание чат-бота, который может выполнять действия.

Если вы хотите углубиться в детали, вот несколько вещей, на которые стоит обратить внимание:

* **Потоковая передача:** потоковая передача **крайне важна** для чат-приложений.
* **Как добавить историю сообщений:** чтобы глубже изучить все, что связано с историей сообщений.