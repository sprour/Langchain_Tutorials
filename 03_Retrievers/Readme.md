## Хранилища векторов и средства поиска

Данное руководство познакомит вас с абстракциями хранилищ векторов и средств поиска в LangChain. Эти абстракции разработаны для поддержки поиска данных - из (векторных) баз данных и других источников - для интеграции с рабочими процессами LLM. Они важны для приложений, которые извлекают данные для последующего анализа в рамках логического вывода модели, как в случае с генерацией с дополнением извлечения (RAG) (см. наше руководство по RAG здесь).

### Концепции

Данное руководство посвящено поиску текстовых данных. Мы рассмотрим следующие концепции:

* Документы;
* Хранилища векторов;
* Средства поиска (Retrievers).

### Настройка

#### Jupyter Notebook

Это и другие руководства, пожалуй, удобнее всего запускать в блокноте Jupyter. Инструкции по установке см. здесь.

### Установка

Для этого руководства требуются пакеты langchain, langchain-chroma и langchain-openai:

**Pip**
```bash
pip install langchain langchain-chroma langchain-openai
```

**Conda**
```bash
conda install -c conda-forge langchain langchain-chroma langchain-openai
```

Для получения более подробной информации см. наше Руководство по установке.

### LangSmith

Многие приложения, которые вы создаете с помощью LangChain, будут содержать несколько шагов с несколькими вызовами LLM. По мере того, как эти приложения становятся все более сложными, становится критически важным иметь возможность проверить, что именно происходит внутри вашей цепочки или агента. Лучший способ сделать это - использовать LangSmith.

После регистрации по ссылке выше обязательно настройте переменные среды, чтобы начать регистрацию трассировок:

```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

Или, если вы работаете в блокноте, вы можете установить их следующим образом:

```python
import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

### Документы

LangChain реализует абстракцию `Document`, которая предназначена для представления единицы текста и связанных метаданных. Она имеет два атрибута:

* `page_content`: строка, представляющая содержимое;
* `metadata`: словарь, содержащий произвольные метаданные.

Атрибут `metadata` может содержать информацию об источнике документа, его связи с другими документами и другую информацию. Обратите внимание, что отдельный объект `Document` часто представляет собой фрагмент более крупного документа.

Давайте сгенерируем несколько примеров документов:

```python
from langchain_core.documents import Document

documents = [
Document(
page_content="Собаки - отличные компаньоны, известные своей преданностью и дружелюбием.",
metadata={"source": "mammal-pets-doc"},
),
Document(
page_content="Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.",
metadata={"source": "mammal-pets-doc"},
),
Document(
page_content="Золотые рыбки - популярные домашние животные для начинающих, требующие относительно простого ухода.",
metadata={"source": "fish-pets-doc"},
),
Document(
page_content="Попугаи - умные птицы, способные имитировать человеческую речь.",
metadata={"source": "bird-pets-doc"},
),
Document(
page_content="Кролики - социальные животные, которым нужно много места, чтобы прыгать.",
metadata={"source": "mammal-pets-doc"},
),
]
```
**Ссылка на API:** Document

Здесь мы сгенерировали пять документов, содержащих метаданные, указывающие на три различных "источника".

### Хранилища векторов

Векторный поиск - это распространенный способ хранения неструктурированных данных (например, неструктурированного текста) и поиска по ним. Идея заключается в хранении числовых векторов, связанных с текстом. Получив запрос, мы можем встроить его в вектор той же размерности и использовать метрики векторного сходства для идентификации связанных данных в хранилище.

Объекты `VectorStore` в LangChain содержат методы для добавления текста и объектов `Document` в хранилище, а также для их запроса с использованием различных метрик сходства. Они часто инициализируются с помощью моделей встраивания (embedding models), которые определяют, как текстовые данные переводятся в числовые векторы.

LangChain включает в себя набор интеграций с различными технологиями векторных хранилищ. Некоторые векторные хранилища размещаются у провайдера (например, у различных облачных провайдеров) и требуют для использования специальных учетных данных; некоторые (например, Postgres) работают в отдельной инфраструктуре, которая может запускаться локально или через третью сторону; другие могут работать в памяти для облегчения нагрузки. Здесь мы продемонстрируем использование LangChain VectorStores на примере Chroma, который включает в себя реализацию в памяти.

Чтобы создать экземпляр векторного хранилища, нам часто нужно предоставить модель встраивания, чтобы указать, как текст должен быть преобразован в числовой вектор. Здесь мы будем использовать встраивания OpenAI.

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(
documents,
embedding=OpenAIEmbeddings(),
)
```

**Ссылка на API:** OpenAIEmbeddings

Вызов `.from_documents` здесь добавит документы в хранилище векторов. `VectorStore` реализует методы для добавления документов, которые также можно вызывать после создания экземпляра объекта. Большинство реализаций позволит вам подключиться к существующему хранилищу векторов, например, предоставив клиент, имя индекса или другую информацию. Подробнее см. документацию по конкретной интеграции.

Создав экземпляр `VectorStore`, содержащий документы, мы можем выполнять по нему запросы. `VectorStore` включает методы для запросов:

* Синхронно и асинхронно;
* По строковому запросу и по вектору;
* С возвратом оценок сходства и без;
* По сходству и максимальной маржинальной релевантности (для баланса сходства с запросом к разнообразию в полученных результатах).

Методы, как правило, будут включать список объектов `Document` в своих выходных данных.

### Примеры

**Возврат документов на основе сходства со строковым запросом:**

```python
vectorstore.similarity_search("кот")

[Document(page_content='Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Собаки - отличные компаньоны, известные своей преданностью и дружелюбием.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Кролики - социальные животные, которым нужно много места, чтобы прыгать.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Попугаи - умные птицы, способные имитировать человеческую речь.', metadata={'source': 'bird-pets-doc'})]
```

**Асинхронный запрос:**

```python
await vectorstore.asimilarity_search("кот")

[Document(page_content='Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Собаки - отличные компаньоны, известные своей преданностью и дружелюбием.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Кролики - социальные животные, которым нужно много места, чтобы прыгать.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Попугаи - умные птицы, способные имитировать человеческую речь.', metadata={'source': 'bird-pets-doc'})]
```

**Возврат оценок:**

```python
# Обратите внимание, что провайдеры реализуют разные оценки; Chroma здесь
# возвращает метрику расстояния, которая должна изменяться обратно пропорционально
# сходству.

vectorstore.similarity_search_with_score("кот")


[(Document(page_content='Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.', metadata={'source': 'mammal-pets-doc'}),
0.3751849830150604),
(Document(page_content='Собаки - отличные компаньоны, известные своей преданностью и дружелюбием.', metadata={'source': 'mammal-pets-doc'}),
0.48316916823387146),
(Document(page_content='Кролики - социальные животные, которым нужно много места, чтобы прыгать.', metadata={'source': 'mammal-pets-doc'}),
0.49601367115974426),
(Document(page_content='Попугаи - умные птицы, способные имитировать человеческую речь.', metadata={'source': 'bird-pets-doc'}),
0.4972994923591614)]
```

**Возврат документов на основе сходства с вектором встроенного запроса:**

```python
embedding = OpenAIEmbeddings().embed_query("кот")

vectorstore.similarity_search_by_vector(embedding)

[Document(page_content='Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Собаки - отличные компаньоны, известные своей преданностью и дружелюбием.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Кролики - социальные животные, которым нужно много места, чтобы прыгать.', metadata={'source': 'mammal-pets-doc'}),
Document(page_content='Попугаи - умные птицы, способные имитировать человеческую речь.', metadata={'source': 'bird-pets-doc'})]
```

**Узнать больше:**

* Ссылка на API
* Руководство по эксплуатации
* Документация по конкретной интеграции

### Средства поиска (Retrievers)

Объекты `VectorStore` в LangChain не являются подклассом `Runnable`, поэтому их нельзя напрямую интегрировать в цепочки LangChain Expression Language.

`Retrievers` в LangChain являются `Runnable`, поэтому они реализуют стандартный набор методов (например, синхронные и асинхронные операции `invoke` и `batch`) и предназначены для включения в цепочки LCEL.

Мы можем создать простую версию этого самостоятельно, не создавая подкласс `Retriever`. Если мы выберем метод, который мы хотим использовать для извлечения документов, мы можем легко создать исполняемый объект. Ниже мы построим его вокруг метода `similarity_search`:

```python
from typing import List

from langchain_core.documents import Document
from langchain_core.runnables import RunnableLambda

retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1) # выбрать лучший результат

retriever.batch(["кот", "акула"])
```

**Ссылка на API:** Document | RunnableLambda

```
[[Document(page_content='Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.', metadata={'source': 'mammal-pets-doc'})],
[Document(page_content='Золотые рыбки - популярные домашние животные для начинающих, требующие относительно простого ухода.', metadata={'source': 'fish-pets-doc'})]]
```

Векторные хранилища реализуют метод `as_retriever`, который генерирует `Retriever`, а именно `VectorStoreRetriever`. Эти средства поиска включают в себя специальные атрибуты `search_type` и `search_kwargs`, которые определяют, какие методы базового хранилища векторов вызывать и как их параметризовать. Например, мы можем повторить вышесказанное следующим образом:

```python
retriever = vectorstore.as_retriever(
search_type="similarity",
search_kwargs={"k": 1},
)

retriever.batch(["кот", "акула"])

[[Document(page_content='Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.', metadata={'source': 'mammal-pets-doc'})],
[Document(page_content='Золотые рыбки - популярные домашние животные для начинающих, требующие относительно простого ухода.', metadata={'source': 'fish-pets-doc'})]]
```

`VectorStoreRetriever` поддерживает типы поиска `"similarity"` (по умолчанию), `"mmr"` (максимальная маржинальная релевантность, описанная выше) и `"similarity_score_threshold"`. Мы можем использовать последний, чтобы отсекать документы, выводимые средством поиска, по оценке сходства.

Средства поиска можно легко интегрировать в более сложные приложения, такие как приложения с дополнением извлечения (RAG), которые объединяют данный вопрос с извлеченным контекстом в подсказку для LLM. Ниже мы покажем минимальный пример.

**OpenAI** | **Anthropic** | **Google** | **Cohere** | **Fireworks** | **AI21** | **Mistral** | **TogetherAI**

```python
pip install -qU langchain-openai

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo-0125")

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

message = """
Ответьте на этот вопрос, используя только предоставленный контекст.

{question}

Контекст:
{context}
"""

prompt = ChatPromptTemplate.from_messages([("human", message)])

rag_chain = {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
```

**Ссылка на API:** ChatPromptTemplate | RunnablePassthrough

```python
response = rag_chain.invoke("расскажи мне о кошках")

print(response.content)

Кошки - независимые домашние животные, которые часто наслаждаются своим собственным пространством.
```

**Узнать больше:**

Стратегии поиска могут быть богатыми и сложными. Например:

* Мы можем выводить жесткие правила и фильтры из запроса (например, "используя документы, опубликованные после 2020 года");
* Мы можем возвращать документы, которые каким-то образом связаны с извлеченным контекстом (например, через некоторую таксономию документов);
* Мы можем генерировать несколько встраиваний для каждой единицы контекста;
* Мы можем объединять результаты нескольких средств поиска;
* Мы можем назначать веса документам, например, чтобы придать больший вес более поздним документам.

В разделе "Средства поиска" руководства по эксплуатации рассматриваются эти и другие встроенные стратегии поиска.

Также несложно расширить класс `BaseRetriever`, чтобы реализовать пользовательские средства поиска. См. наше руководство по эксплуатации здесь.